# YAML file listing config parameters
# Paths
paths:
  base: ****************UPDATE_ME_TO_YOUR_CLONE_PATH******************
  data: /code/data/
  patchparser: /data/patchparser-data/
  advisory: /data/advisory-data/
  results: /data/model-results-few-examples/
  rag_db: /data/rag-db/

  template: /code/llm/templates/
  few_shot_system_prompts: /code/prompts/few-shot/system-prompt/
  zero_shot_system_prompts: /code/prompts/zero-shot/system-prompt/
  user_template: /code/prompts/user-template/
  example_template: /code/prompts/few-shot/example-template/
  
# Data
data:
  patchparser: patchparser-data-2023-10-31.csv
  go_advisory: govulndb-2023-10-26.csv
  go_similar_description: govulndb-similar-descriptions-2023-10-31.csv
  test: True # Set to False if you want to run across all data
  test_amount: 5
  debug_test_range: False # Specify a specific test range of idx's
  debug_test_start: 32
  debug_test_end: 36
  clean_advisory_details: True # removes extra & new lines from the details string
  tp_cot_examples: govulndb-cot-examples-tp-2023-10-31.csv # created from cot-generate-tp-example.py
  fp_cot_examples: govulndb-cot-examples-fp-2023-10-31.csv # created from cot-generate-fp-example.py

  # RAG DBs
  rag:
    # Advisory data embeddings
    faiss_advisory: advisory-faiss-index-2023-10-26.bin
    faiss_advisory_map: advisory-faiss-index-map-2023-10-26.csv
    # TP Git-hunk data embeddings
    faiss_tp_git_hunk: tp-git-hunk-faiss-index-2023-10-26.bin
    faiss_tp_git_hunk_map: tp-git-hunk-faiss-index-map-2023-10-26.csv
    # FP Git-hunk data embeddings
    faiss_fp_git_hunk: fp-git-hunk-faiss-index-2023-10-26.bin
    faiss_fp_git_hunk_map: fp-git-hunk-faiss-index-map-2023-10-26.csv
    # All Git-hunk data embeddings
    faiss_complete_git_hunk: complete-git-hunk-faiss-index-2023-10-26.bin
    faiss_complete_git_hunk_map: complete-git-hunk-faiss-index-map-2023-10-26.csv

# Set the different CodeLlama Types
models:
  base: ****************UPDATE_ME_TO_YOUR_MODEL_CLONE_PATH******************
  model_name: CodeLlama-34b-Instruct-hf/ 
  # Mixtral-8x7B-Instruct-v0.1 CodeLlama-7b-Instruct-hf deepseek-coder-6.7b-instruct
  config_file: v17_llama34b_cot_few_shot.yaml
  
  # set the RAG embeddings models
  rag:
    text_embedding_model: bge-small-en-v1.5/
    code_embedding_model: codet5p-110m-embedding/
    rag_device: cpu # always keep at CPU


# Set the prompts to use in the model
prompts:
  paradigm: [few] # [zero, few]
  template_zero_shot: template_zero_shot_llama.yaml
  template_few_shot: template_few_shot_llama.yaml

  # these will be dynamically set
  # e.g., /prompts/few-shot/system-prompts/vuln-detailed-explain-user-v3.txt
  system_prompt_type: [vuln-detailed-explain] #[basic, detailed, vuln-detailed, vuln-detailed-explain]
  system_prompt_version: 3

  # these will be dynamically set
  # e.g., /prompts/few-shot/example-template/vuln-detailed-explain-user-v3.txt
  example_template_version: 3
  example_type: [git-hunk] # [static, random, description, git-hunk]
  example_amount: [5] # [1, 3, 5, 7, 10, 20]
  example_static_tp: [0, 50, 75, 90, 95, 150, 185, 200, 250, 260] # these are row numbers in the TP Data DF, not used
  example_static_fp: [50, 85, 90, 97, 100, 115, 155, 195, 215, 300] # these are row numbers in the FP Data DF, not used
  example_order: [alt] # determine the order of the examples [tp, fp, alt, complete, alt_dependent, random_static]
  example_sort: [descending] # [ascending, descending] ascending = most similar closer to user input
  example_sort_last_random: False # True/False. True randomly sorts the last examples before the user prompt. 
  example_sort_last_random_amount: 2 # How many of the last samples to sort. Must be less than example_amount

  # these will be dynamically set
  # e.g., /prompts/user-template/vuln-detailed-explain-user-v3.txt
  user_template_version: 3

# Set generation configs
# Great info on this -> https://huggingface.co/blog/how-to-generate
# Defaults from Meta: 
gen_cfg:
  device_map: auto # [auto, cuda:0]
  batch_device_map: cuda # [cuda, cuda:0]
  load_in_8bit: False
  load_in_4bit: True
  do_sample: False # False = greedy search, take highest probability
  max_new_tokens: 1000
  top_p: 0.9
  top_k: 50
  temperature: 0.6
  repetition_penalty: 1.15
  return_dict_in_generate: True
  output_scores: True
  num_return_sequences: 1


# Log prob adjustments within utils/model_helper/combine_results
# Not actually used
log_prob:
  use_log_probs: False # set true if you want to use log probs. Used in model_hlper/aggregate_save_results
  p_true_prior: 0.7
  p_false_prior: 0.3
  bayesian_threshold: 0.5
  true_threshold: 0.5
  diff_threshold: 0.1


# determine if we want to save results
results:
  save_results: False
  full_results: full_results_config_{CONFIG_VER}.csv
  agg_results: agg_results_config_{CONFIG_VER}.csv
  rag_logs: False # log and the retrival FAISS scores


# Weights and Biases Logging
# Logging Weights and Biases
wandb:
  mode: 'online'  # ['disabled', 'offline', 'online']
  project: your-project
  group: cot_v0
  log_results: False
